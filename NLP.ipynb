{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NLP Notebook (Twitter, Sentiment Analysis and A Shakespeare Generator)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1 - Twitter NLP\n",
    "\n",
    "You just signed up for PyDataLondon and you are super excited about it! Since you hear that measuring twitter sentiment is all the craze these days (be it for speculating in the stock market, or identifying a viral product), you decide that you also want in. Let's try to apply some NLP (natural language processing) goodness to analyze #PyDataLondon tweets!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the data\n",
    "\n",
    "First grab the data that we've downloaded for you.\n",
    "The data is saved in the [pickle format](https://docs.python.org/3/library/pickle.html#data-stream-format).  \n",
    "Don't be worried if you don't understand this part - it's just to set you up for the main parts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "with open('./datasets/twitter_data.pkl', 'rb') as pickled_file:\n",
    "    twitter_data = pickle.load(pickled_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's check how many tweets we have.  \n",
    "Note, that iPython automatically displays the last output in the cell,\n",
    "so it is enough to write `len(tweets_list)` instead of `print(len(tweets_list))`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(twitter_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Explore the data\n",
    "\n",
    "Let's see what a tweet looks like"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Each tweet is represented by a dictionary with a following fields:\n",
    "twitter_data[0].keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Text of the first tweet\n",
    "twitter_data[0]['text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can extract the text from the tweets\n",
    "tweets_text = [tweet['text'] for tweet in twitter_data]\n",
    "\n",
    "# To see if it works, print out the first 10 tweets\n",
    "tweets_text[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's also take a look at the number of characters in a tweets. This dataset is from before Twitter changed their character limit, so you would expect there to be mostly < 140 character tweets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweet_lengths = [len(text) for text in tweets_text]\n",
    "\n",
    "# Let's print legth of the first 10 tweets\n",
    "tweet_lengths[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can have better understanding of our data if we plot a histogram instead of looking at the list of numbers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "# Get notebook to show graphs\n",
    "%pylab inline\n",
    "\n",
    "# Use new pretty style of plots\n",
    "matplotlib.style.use('ggplot')\n",
    "\n",
    "# Because data scientists hate charts with no labels, let's add them :D\n",
    "plt.ylabel('frequency')\n",
    "plt.xlabel('number of characters in tweet')\n",
    "\n",
    "# We can transform our list of tweet lengths from list to pandas Series\n",
    "# it will let us to use hist() method to create histogram\n",
    "pd.Series(tweet_lengths).hist(bins=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# What's the average number of characters? What's the maximum or minimum?\n",
    "# We will again use pandas Series instead of the python builtin type (list)\n",
    "# It will allow us to use the describe method\n",
    "tweet_lengths_series = pd.Series(tweet_lengths)\n",
    "\n",
    "tweet_lengths_series.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Words counts\n",
    "\n",
    "We are going to use a technique called [word vectors](http://www.eecs.qmul.ac.uk/~dm303/static/eecs_open14/eecs_open14.pdf) to find out which words are most commonly used together with which other words. On the way to doing that, we will also see some very cool visualizations for word counts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "word_count = defaultdict(int)\n",
    "\n",
    "for tweet in tweets_text:\n",
    "    for word in tweet.split():\n",
    "        word_count[word] += 1\n",
    "\n",
    "# Count the words used in our tweets\n",
    "print('{} unique words'.format(len(word_count)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here is a python standard library feature that is quite cool!\n",
    "from collections import Counter\n",
    "\n",
    "words = Counter(word_count)\n",
    "print(words.most_common(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualization\n",
    "\n",
    "If you were asked to find the best chart to visualize word counts, how would you do it? Here's a cool little non-standard library that you should be able to install with a single command. Python is amazing!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from wordcloud import WordCloud\n",
    "\n",
    "temp = {'a': 3, 'b': 1}\n",
    "\n",
    "wordcloud = WordCloud(width=800, height=600).generate_from_frequencies(words)\n",
    "plt.imshow(wordcloud)\n",
    "plt.axis(\"off\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Word clouds are so coool. Let's make the picture take up the whole screen, so we can stare at it __IN ALL ITS GLORY__ :D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def enlarge(multiplier=2):\n",
    "    \"\"\"If you want to understand more about this function, refer to the data visualization notebook.\"\"\"\n",
    "    figure = plt.gcf()\n",
    "    original_width, original_height = figure.get_size_inches()\n",
    "    new_size = (original_width * multiplier, original_height * multiplier)\n",
    "    figure.set_size_inches(new_size)\n",
    "\n",
    "enlarge()\n",
    "plt.imshow(wordcloud)\n",
    "plt.axis(\"off\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data cleanup\n",
    "\n",
    "Let's get back on track again... Too much chart porn is bad for you after all.\n",
    "\n",
    "First, let's do some long overdue data cleanup that we spotted from the word cloud. We probably don't care about retweets, prepositions etc. And on that note, we also probably don't care about the words which only occur a couple times."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# It is good practice to exclude the most common words,\n",
    "# like articles (the, a, ...), prepositions (on, by, ...) or some abreviations (rt - retweeted)\n",
    "exclude_words = {\n",
    "    'rt', 'to', 'for', 'the', 'with', 'at', 'via', 'on', 'if', 'by', 'how', 'are', 'this'\n",
    "    'do', 'into', 'or', '-', 'you', 'is', 'a', 'i', 'it', 'in', 'and', 'of', 'from', '&gt'\n",
    "}\n",
    "\n",
    "word_count_filtered = {k: v for k, v in word_count.items() if k.lower() not in exclude_words}\n",
    "\n",
    "# Let's represent the word_count_filtered as pandas DataFrame\n",
    "words = pd.DataFrame.from_dict(word_count_filtered, orient='index').rename(columns={0: 'frequency'})\n",
    "\n",
    "# The results are as following\n",
    "words.head(15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We want to limit our vocabulary to only the most common words\n",
    "limit = 30\n",
    "\n",
    "shortened_list = words[words.frequency > limit]\n",
    "print(\n",
    "    'If we limit the words to any word that at least occurs {} times, '\n",
    "    'we are left with {} words (from {} words)'.format(\n",
    "        limit, len(shortened_list), len(words)\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Colocation/co-occurrence frequency\n",
    "\n",
    "Now we are finally all set to figure out the question we had previously posed: if a word is in the tweet, how frequently do these other words also show up in the tweet?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First, let's create a DataFrame filled with zeros\n",
    "occurrence_frequency = pd.DataFrame(0, index=shortened_list.index.values, columns=shortened_list.index.values)\n",
    "\n",
    "# Sanity check (let's see if we succeeded, by printed the first blok of the matrix)\n",
    "occurrence_frequency.iloc[:5, :5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Next, let's remove all the unncessary words from our tweets\n",
    "allowed_words = occurrence_frequency.index\n",
    "\n",
    "cleaned_tweets = []\n",
    "for text in tweets_text:\n",
    "    words_in_one_tweet = text.split()\n",
    "    cleaned_tweets.append([w for w in words_in_one_tweet if w in allowed_words])\n",
    "\n",
    "# To check if everything works, we print the first 10 tweets\n",
    "# we should see only the most common words\n",
    "cleaned_tweets[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A triple for-loop to add up and fill in the counts for each word vis-a-vis other words\n",
    "for word_list in cleaned_tweets:\n",
    "    for word in word_list:\n",
    "        for other_word in word_list:\n",
    "            occurrence_frequency[word][other_word] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's display our results (first 10 lines)\n",
    "occurrence_frequency.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Great! Now we have everything setup and we are ready to look at the [cosine similarity](https://en.wikipedia.org/wiki/Cosine_similarity) between different words.\n",
    "\n",
    "We are thinking of each word as a n-dimensional vector (where each dimension is the co-occurence frequency for another specific word). The cosine similarity basically looks and says, \"hey `word_a` co-occurs a lot with `word_b` but does not appear with `word_c`. Oh hey, `word_d` also co-occurs a lot with `word_b` but not with `word_c`. I guess that `word_a` and `word_d` must be quite similar then.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.spatial.distance import pdist, squareform\n",
    "\n",
    "cosine_distances = squareform(pdist(occurrence_frequency, metric='cosine'))\n",
    "cosine_distances.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's look at the top left corner of our array\n",
    "cosine_distances[:5,:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can see that the distances between any word and itself is 0.\n",
    "Let's flip it around for a second and look at similarity instead."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cosine_similarities_array = np.exp(-cosine_distances)\n",
    "similarity = pd.DataFrame(\n",
    "    cosine_similarities_array, \n",
    "    index=occurrence_frequency.index, \n",
    "    columns=occurrence_frequency.index\n",
    ")\n",
    "similarity.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now you can see that any word is 100% similar with itself.  \n",
    "Well that is great and all, but how would you visualize word similarity?  \n",
    "It turns out that scikit learn has just the tool for us:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import manifold\n",
    "\n",
    "# see http://scikit-learn.org/stable/modules/manifold.html#multidimensional-scaling\n",
    "mds = manifold.MDS(n_components=2, dissimilarity='precomputed')\n",
    "words_in_2d = mds.fit_transform(cosine_distances)\n",
    "words_in_2d[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[MDS](https://en.wikipedia.org/wiki/Multidimensional_scaling) allows us to go from the n by n matrix down to a more manageable lower-dimension representation of the n words.  \n",
    "In this case, we choose a 2-d representation, which allows us to..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make a bubble chart\n",
    "counts = [word_count[word] for word in occurrence_frequency.index.values]\n",
    "plt.scatter(x=words_in_2d[:,0], y=words_in_2d[:,1], s=counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's enlarge it and add labels\n",
    "enlarge()\n",
    "important_words = words[words.frequency > 80].index.values\n",
    "for word in important_words:\n",
    "    idx = occurrence_frequency.index.get_loc(word)\n",
    "    plt.annotate(word, xy=words_in_2d[idx], xytext=(0,0), textcoords='offset points')\n",
    "plt.scatter(x=words_in_2d[:,0], y=words_in_2d[:,1], s=counts, alpha=0.3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That's cool- you can see there is:\n",
    "- a cluster with monty + python\n",
    "- a cluster of (I'm guessing) Spanish words\n",
    "- a cluster of data science / big data / machine learning / data analytics, which weirdly also contains @kirkdborne. Checking his twitter, it turns out he posts a lot about data science!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dig Deeper\n",
    "\n",
    "If you've gotten to here, a big congratulations on finishing the first part of this tutorial!\n",
    "\n",
    "If you stil have time, here are a couple suggestions for you to work on:\n",
    "\n",
    "- Try to write your own code to download twitter tweets. [Here](http://adilmoujahid.com/posts/2014/07/twitter-analytics/) is a guide that is quite comprehensive. You will have to setup a twitter developer's account, create an app and get an api token first though.\n",
    "- Try to use what we have developed so far to create your own search algorithm. eg: search for all the tweets that has to do with machine learning (and make it smart enough to automatically show anything related to data science, big data, data analytics etc)\n",
    "- We kept bumping up against resource limits, especially during the triple for loop when filling out the occurrence_frequency counts. Given n tweets, there are probably k*n words, and so it has (very very roughly) a [computation complexity](https://en.wikipedia.org/wiki/Big_O_notation) of O(n^3). Most of the other computations we did were mainly O(kn). Can we rewrite the code to make it better?\n",
    "- For this last scatter plot we just generated showing which words are frequently used with which other words, can we use a clustering algorithm to color them, so that we can see the clusters that we observed more clearly?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.core.display import HTML\n",
    "HTML(\"\"\"\n",
    "    <blockquote class=\"twitter-tweet\" data-lang=\"en\"><p lang=\"en\" dir=\"ltr\">Fun with Clusters at <a href=\"https://twitter.com/hashtag/PyDataLondon?src=hash&amp;ref_src=twsrc%5Etfw\">#PyDataLondon</a> <a href=\"https://t.co/j42lbx4kyx\">pic.twitter.com/j42lbx4kyx</a></p>&mdash; Lewis Oaten (@lewisoaten) <a href=\"https://twitter.com/lewisoaten/status/728548835082047489?ref_src=twsrc%5Etfw\">May 6, 2016</a></blockquote>\n",
    "    <script async src=\"https://platform.twitter.com/widgets.js\" charset=\"utf-8\"></script>\n",
    "\"\"\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Turns out you can color the clusters!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2 - Sentiment Analysis\n",
    "\n",
    "In this part we will use textblob to determine the sentiment of the tweets. Textblob already has ready-trained classifiers that we can use for this purpose, so it is quite plug and play.\n",
    "\n",
    "First, let's make sure we understand how it works:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from textblob import TextBlob\n",
    "\n",
    "# Let's check a polarity of a positive sentence (try some other sentences as well!)\n",
    "blob = TextBlob(\"The life is good.\")\n",
    "blob.polarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Nowe we can check a polarity of a negative sentence (try some other sentences as well!)\n",
    "blob = TextBlob(\"The life is tough.\")\n",
    "blob.polarity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For textblob, we also need to clean the tweets to remove links and special characters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def clean_tweet(tweet):\n",
    "    return ' '.join(re.sub(\"(@[A-Za-z0-9]+)|([^0-9A-Za-z \\t])|(\\w+:\\/\\/\\S+)\", \" \", tweet).split())\n",
    "\n",
    "cleaned_text = [clean_tweet(tweet['text']) for tweet in twitter_data]\n",
    "\n",
    "cleaned_text[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's check the sentiment of each tweet!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets_with_polarity = [(TextBlob(t).polarity, t) for t in cleaned_text]\n",
    "    \n",
    "# let's check the results\n",
    "tweets_with_polarity[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the most positive tweets\n",
    "sorted(tweets_with_polarity, key=lambda tup: tup[0])[-10:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the most negative tweets\n",
    "sorted(tweets_with_polarity, key=lambda tup: tup[0])[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dig Deeper\n",
    "Check out [this tutorial](https://dev.to/rodolfoferro/sentiment-analysis-on-trumpss-tweets-using-python-) if you are interested."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3 - A Shakespeare Generator\n",
    "\n",
    "In part 1, we looked at word count / word level analytics. Inspired by the [unreasonable effectiveness](http://nbviewer.jupyter.org/gist/yoavg/d76121dfde2618422139) of character-level language models, let's try to use a Maximum Likelihood Character Level Language Model to generate Shakespeare!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First we need a large body of text\n",
    "!wget http://cs.stanford.edu/people/karpathy/char-rnn/shakespeare_input.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's see what the file contains\n",
    "\n",
    "with open(\"shakespeare_input.txt\") as f:\n",
    "    shakespeare = f.read()\n",
    "print(shakespeare[:300])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "def train_char_lm(data, order=4):\n",
    "    \"\"\"Train the Maximum Likelihood Character Level Language Model.\"\"\"\n",
    "    language_model = defaultdict(Counter)\n",
    "    \n",
    "    # we add special characters at the beginning of the text to get things started\n",
    "    padding = \"~\" * order\n",
    "    data = padding + data\n",
    "    \n",
    "    # count how many times a given letter follows after a particular n-char history.\n",
    "    for i in range(len(data) - order):\n",
    "        history, char = data[i:i + order], data[i + order]\n",
    "        language_model[history][char] += 1\n",
    "\n",
    "    # we normalize our results\n",
    "    normalized = {hist: normalize(chars) for hist, chars in language_model.items()}\n",
    "    return normalized\n",
    "\n",
    "\n",
    "def normalize(counter):\n",
    "    \"\"\"Normalize counter by the sum of all values.\"\"\"\n",
    "    sum_of_values = float(sum(list(counter.values())))\n",
    "    return [(key, value/sum_of_values) for key, value in counter.items()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's us train our model!\n",
    "language_model = train_char_lm(shakespeare, order=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check how the model look like\n",
    "list(language_model.items())[:6]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It means, that after `Firs`, we always get `t` with probability 1. But after `First`, we might see a space with probability 0.83, or comma with probability 0.082 etc.\n",
    "\n",
    "Let's us check which letter is the most probable after `hous`. Since we generated a model with order 4, we can look only at last 4 letters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Other example\n",
    "language_model['hous']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The most probable, as expected, is `e` (house).\n",
    "\n",
    "Why `a`?  Becuase `hous` can be a part of the `thousands`.\n",
    "\n",
    "Play around with this!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's use the model to generate some Shakespearean!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from random import random\n",
    "\n",
    "def generate_letter(model, history, order):\n",
    "    \"\"\"Generate next letter with given probabilities.\"\"\"\n",
    "    history = history[-order:]\n",
    "    probabilities = model[history]\n",
    "    x = random()\n",
    "    for character, prob in probabilities:\n",
    "        x = x - prob\n",
    "        if x <= 0:\n",
    "            return character"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_text(model, order, nletters=1000):\n",
    "    \"\"\"Generate new text using our model.\"\"\"\n",
    "    # Use the special character to get things started\n",
    "    history = \"~\" * order\n",
    "    out = []\n",
    "    for i in range(nletters):\n",
    "        c = generate_letter(model, history, order)\n",
    "        history = history[-order:] + c\n",
    "        out.append(c)\n",
    "    return \"\".join(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(generate_text(language_model, 4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is amazing how such a simple model is enough to generate text that has a structure of a play, with capitalized character names in the script etc.\n",
    "\n",
    "Run the above again and try generating more text!\n",
    "\n",
    "We can also increase the model order to get even better results. However, it will take exponentially more time to create the model. However, once we have the model, generating new text should be quite fast."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Finally, check the order 10. It can take a while...\n",
    "language_model = train_char_lm(shakespeare, order=10)\n",
    "print(generate_text(language_model, 10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dig Deeper\n",
    "\n",
    "- Try to repeat the above using tweets instead of Shakespeare text. Does it work? Is the text in tweets long enough to train our model well?\n",
    "- Our model seems to be impressive. But is the generated text really original? If we trained the model to an order of 100 or even 1000 on a really powerful machine, what would the output be if we tried to generate some text?\n",
    "- Believe it or not, there are better methods out there. If you are interested, check out [this article](http://karpathy.github.io/2015/05/21/rnn-effectiveness/) by Andrej Karpathy describing how to\n",
    "generating Shakespeare-like text using Recurrent Neural Networks.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Environment (pydata)",
   "language": "",
   "name": "pydata"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
