{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Backstory and the Setup\n",
    "\n",
    "You just signed up for PyDataLondon and you are super excited about it! Since you hear that measuring twitter sentiment is all the craze these days (be it for speculating in the stock market, or identifying a viral product), you decide that you also want in. Let's try to apply some NLP (natural language processing) goodness to analyze #PyDataLondon tweets!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# grab the data that we've downloaded for you\n",
    "# again, don't be worried if you don't understand this part- it's just to set you up for the main parts\n",
    "import pickle\n",
    "\n",
    "with open('./datasets/twitter_data.pkl', 'rb') as pickled_file:\n",
    "    tweets_list = pickle.load(pickled_file)\n",
    "# quick sanity check\n",
    "print(len(tweets_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# let's see what a tweet looks like\n",
    "tweets_list[0].keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "tweets_series = pd.Series(tweets_list)\n",
    "tweets = tweets_series.apply(lambda x: x['text'].lower())\n",
    "# print out the first 5 stations just as a sanity check\n",
    "tweets.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A Detour\n",
    "\n",
    "If you may be pressed for time, perhaps skip to the next section (Back on Track) and come back here later.\n",
    "\n",
    "For those of you who tried collecting your own twitter stream and didn't filter it as aggressively, you may notice that importing it and later manipulating it may have taken a long time. Exactly how memory intensive is this?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "twitter_char_limit = 140\n",
    "worst_case_utf8_bytes_per_char = 4\n",
    "ram = twitter_char_limit * worst_case_utf8_bytes_per_char * len(tweets)\n",
    "\n",
    "print('Series could take up to {:.2f} MB of memory'.format(ram / 1024. / 1024.))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Thanksfully, UTF-8 is variable length, and works similar to [Huffman Coding](https://en.wikipedia.org/wiki/Huffman_coding), which helps cut down the number of bytes per char significantly. Also people's tweets don't take up the max 140 characters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take a quick detour to look at the number of characters in a tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tweet_lengths = tweets.apply(lambda text: len(text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's plot a histogram of the character lengths!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "# use new pretty plots\n",
    "matplotlib.style.use('ggplot')\n",
    "# get notebook to show graphs\n",
    "%pylab inline\n",
    "\n",
    "# because data scientists hate charts with no labels :D\n",
    "plt.ylabel('frequency')\n",
    "plt.xlabel('number of characters in tweet')\n",
    "tweet_lengths.hist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# what's the average number of characters?\n",
    "import numpy as np\n",
    "tweet_lengths.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "average_chars_per_tweet = tweet_lengths.mean()\n",
    "average_utf8_bytes_per_char = 2  # random approximation\n",
    "ram = average_chars_per_tweet * average_utf8_bytes_per_char * len(tweets)\n",
    "\n",
    "print('Expect series to take about {:.2f} MB of memory'.format(ram / 1024. / 1024.))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is another way to roughly check how much memory is being used:\n",
    "\n",
    "Pandas can store objects into hdf5 files (similar to pickle in python)\n",
    "You can then load the object back out from the permanent file into memory later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# make sure you have `pip3.5 install --user tables`\n",
    "tweets.to_hdf('tweets.h5', key='tweets')\n",
    "# you can access that same file and take the data back out\n",
    "data = pd.read_hdf('tweets.h5')\n",
    "data.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# OMG what is this magic- you can access the linux commandline toolkit from the notebook\n",
    "!ls -lh datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice what happens to the file size of tweets.h5 if you run the store command more than once."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for ii in range(3):\n",
    "    with pd.HDFStore('datasets/tweets.h5') as store:\n",
    "        store.put('tweets', tweets)\n",
    "        print(store)\n",
    "    !ls -lh datasets/tweets.h5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# in fact, look at what happens even if you delete the object\n",
    "with pd.HDFStore('datasets/tweets.h5') as store:\n",
    "    store.remove('tweets')\n",
    "    print(store)\n",
    "!ls -lh datasets/tweets.h5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While the HDF5 format used here is not exactly a completely read-only and immutable file system, this characteristic is typical and extremely important for a lot of distributed file systems (Hadoop's [HDFS](https://www-01.ibm.com/software/data/infosphere/hadoop/hdfs/), Google's [GFS](https://en.wikipedia.org/wiki/Google_File_System)... etc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Back on Track\n",
    "\n",
    "Well. Afer that huge detour, let's go back to analyzing the tweets. We are going to use a technique called [word vectors](http://www.eecs.qmul.ac.uk/~dm303/static/eecs_open14/eecs_open14.pdf), and try to find out which words are most commonly used with other words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "word_count = defaultdict(int)\n",
    "\n",
    "for tweet in tweets.values:\n",
    "    for word in tweet.split():\n",
    "        word_count[word] += 1\n",
    "\n",
    "print('{} unique words'.format(len(word_count)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# let's show off another python standard library feature\n",
    "from collections import Counter\n",
    "\n",
    "words = Counter(word_count)\n",
    "print(words.most_common(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you were asked to find the best chart to visualize word counts, what would your choice be?\n",
    "\n",
    "I know what my choice is.\n",
    "\n",
    "Here's a cool little non-standard library that you should be able to install with a single command.\n",
    "Python is amazing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# pip3.5 install --user wordcloud\n",
    "from wordcloud import WordCloud\n",
    "wordcloud = WordCloud(width=800, height=600).generate_from_frequencies(words.items())\n",
    "plt.imshow(wordcloud)\n",
    "plt.axis(\"off\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Word clouds are so coool. In view of that, let's make the picture take up the whole screen, so we can stare at it __IN ALL ITS GLORY__ :D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def enlarge(multiplier=2):\n",
    "    # if you want to understand more about this function, refer to the data visualization notebook\n",
    "    params = plt.gcf()\n",
    "    original_width, original_height = params.get_size_inches()\n",
    "    new_size = (original_width * multiplier, original_height * multiplier)\n",
    "    params.set_size_inches(new_size)\n",
    "\n",
    "enlarge()\n",
    "plt.imshow(wordcloud)\n",
    "plt.axis(\"off\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ahem\n",
    "Let's get back on track again... Too much chart porn is bad for you after all."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, let's do some long overdue data cleanup that we spotted from the word cloud. We probably don't care about retweets, prepositions etc. And on that note, we also probably don't care about the words which only occur a couple times."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "exclude_words = [\n",
    "    'rt', 'to', 'for', 'the', 'with', 'at', 'via', 'on', 'if', 'by', 'how', 'are', 'this'\n",
    "    'do', 'into', 'or', '-', 'you', 'is', 'a', 'i', 'it', 'in', 'and', 'of', 'from'\n",
    "]\n",
    "word_count_filtered = {k: v for k, v in word_count.items() if k not in exclude_words}\n",
    "words = pd.DataFrame.from_dict(word_count_filtered, orient='index').rename(columns={0: 'frequency'})\n",
    "words.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "limit = 30\n",
    "shortened_list = words[words.frequency > limit]\n",
    "print(\n",
    "    'If we limit the words to any word that at least occurs {} times, '\n",
    "    'we are left with {} words (from {} words)'.format(\n",
    "        limit,\n",
    "        len(shortened_list), len(words)\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's calculate the colocation/co-occurrence frequency:\n",
    "\n",
    "ie. if this word is in the tweet, how frequent is it that these other words are also in the tweet?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# First, let's create a DataFrame filled with zeros\n",
    "occurrence_frequency = pd.DataFrame(0, index=shortened_list.index.values, columns=shortened_list.index.values)\n",
    "# sanity check again\n",
    "occurrence_frequency.iloc[:5, :5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# next, let's remove all the unncessary words \n",
    "cleaned_tweets = tweets.apply(lambda tweet: [word for word in tweet.split() if word in occurrence_frequency.index])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# a triple for-loop to add up and fill in the counts for each word vis-a-vis other words\n",
    "for word_list in cleaned_tweets.values:\n",
    "    for word in word_list:\n",
    "        for other_word in word_list:\n",
    "            occurrence_frequency[word][other_word] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "occurrence_frequency.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Great! Now we have everything setup and we are ready to look at the [cosine similarity](https://en.wikipedia.org/wiki/Cosine_similarity) between different words.\n",
    "\n",
    "We are thinking of each word as a n-dimensional vector (where each dimension is the co-occurence frequency for another specific word) The cosine similarity basically looks and says, \"hey `word_a` co-occurs a lot with `word_b` but does not appear with `word_c`. Oh hey, `word_d` also co-occurs a lot with `word_b` but not with `word_c`. I guess that `word_a` and `word_d` must be quite similar then.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from scipy.spatial.distance import pdist, squareform\n",
    "cosine_distances = squareform(pdist(occurrence_frequency, metric='cosine'))\n",
    "cosine_distances.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "cosine_distances[:5,:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can see that the distances between any word and itself is 0.\n",
    "Let's flip it around for a second and look at similarity instead."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "cosine_similarities_array = np.exp(-cosine_distances)\n",
    "similarity = pd.DataFrame(\n",
    "    cosine_similarities_array, \n",
    "    index=occurrence_frequency.index, \n",
    "    columns=occurrence_frequency.index\n",
    ")\n",
    "similarity.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now you can see that any word is 100% similar with itself."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Well that is great and all, but how would you visualize word similarity?\n",
    "\n",
    "It turns out that scikit learn has just the tool for us:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn import manifold\n",
    "# http://scikit-learn.org/stable/modules/manifold.html#multidimensional-scaling\n",
    "mds = manifold.MDS(n_components=2, dissimilarity='precomputed')\n",
    "words_in_2d = mds.fit_transform(cosine_distances)\n",
    "words_in_2d[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[MDS](https://en.wikipedia.org/wiki/Multidimensional_scaling) allows us to go from the n by n matrix down to a more manageable lower-dimension representation of the n words. In this case, we choose a 2-d representation, which allows us to..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# make a bubble chart\n",
    "\n",
    "counts = [word_count[word] for word in occurrence_frequency.index.values]\n",
    "plt.scatter(x=words_in_2d[:,0], y=words_in_2d[:,1], s=counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "enlarge()\n",
    "important_words = words[words.frequency > 80].index.values\n",
    "for word in important_words:\n",
    "    idx = occurrence_frequency.index.get_loc(word)\n",
    "    plt.annotate(word, xy=words_in_2d[idx], xytext=(0,0), textcoords='offset points')\n",
    "plt.scatter(x=words_in_2d[:,0], y=words_in_2d[:,1], s=counts, alpha=0.3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That's cool- you can see there is:\n",
    "- a cluster with monty + python\n",
    "- a cluster of (I'm guessing) Spanish words\n",
    "- a cluster of data science / big data / machine learning / data analytics, which weirdly also contains @kirkdborne. Checking his twitter, it turns out he posts a lot about data science!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you've gotten to here, a big congratulations on finishing the hardest tutorial of the bunch!\n",
    "\n",
    "If you stil have time, here are a couple suggestions for you to work on:\n",
    "\n",
    "- Try to write your own code to download twitter tweets. Ask me to reference the code I used. [Here](http://adilmoujahid.com/posts/2014/07/twitter-analytics/) is another tutorial that is quite comprehensive. You will have to setup a twitter developer's account, crerate an app and get an api token first.\n",
    "- Try to use what we have developed so far to create your own search algorithm. eg: search for all the tweets that has to do with machine learning (and it knows to shows anything related to data science, big data, data analytics etc)\n",
    "- This was definitely a case where we kept bumping up against resource limits. The triple for loop when filling out the occurrence_frequency counts is a killer- given n tweets, there are probably k*n words, and so it has (very very roughly) a [computation complexity](https://en.wikipedia.org/wiki/Big_O_notation) of O(n^3), compared to most of the other stuff we did, which was mainly O(kn). Can we rewrite the code to make it better?\n",
    "- For the last scatter plot we just generated, user a clustering algorithm to color them, so that we can see the clusters that we just observed more clearly"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.5",
   "language": "python",
   "name": "python3.5"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
